Index: src/tutorial_5/scripts/tutorial5_soccer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/usr/bin/env python\nfrom logging import StringTemplateStyle\nimport random\nimport rospy\nfrom std_msgs.msg import String, Header\nfrom std_srvs.srv import Empty\nfrom naoqi_bridge_msgs.msg import JointAnglesWithSpeed,Bumper,HeadTouch\nfrom naoqi import ALProxy\nfrom sensor_msgs.msg import Image,JointState\nfrom cv_bridge import CvBridge, CvBridgeError\nimport cv2\nimport numpy as np\nimport csv\nimport random\nfrom naoqi import ALProxy\nimport sys\nfrom MDP import *\n\nLEG_MAX = 0.790477\nLEG_MIN = -0.379472\n\nclass tutorial5_soccer:\n    def __init__(self, initial_states):\n        self.jointPub = 0\n        states = range(10)  # number of states (assume discretized leg distance from the hip is 10\n        terminal_state = {0, 9}  ##[hkh] Let assume at the end of leg movement will be end state ## depending on DT Terminal states are differed\n\n        # Initial variables for RL-DT\n        self.exp_ = False\n        self.S_M = set()   # set type to store and check visited state per episode\n        self.initial_state = initial_states\n        ##[hkh] initialize each visit for each state zero for each state\n        self.actions = {\"move_right\": 0, \"move_left\": 0, \"kick\": 0}\n        # self.actions = [\"move_right\", \"move_left\", \"kick\"]\n        self.stateSet = {s: self.actions for s in states}  # =visits: state-action pair is initially zero, no states visited so far\n        self.G_t = 0 # total future reward up to given time t\n        self.train_epsiodes = 100   # random guess\n        self.reward = {\"move_leg\": -1, \"fall\": -20, \"kick_fail\": -2, \"goal\": 20 }\n\n        self.leg_state_abs = 0 #-0.379472 to 0.790477 -> discretized by 10 ~ 0.117 per bin\n        self.leg_state_dis = 10   # 0 - 9, 10 for invalid\n\n        ##[hkh] we have to implement DT for transition probability. DT\n        # (reminder: in DT example, the state should be vaild within the possible range.\n        # e.g. In A=L node, if True for x=0 meaning no movement, then its output is either 0:no movement as true, -1:moved left\n        # In A=R, if x=1: moved right as true, x=0: idle. its output 0 as True or Y=1 as False.\n\n    def discretize_leg(self):\n        self.leg_state_dis = np.round((self.leg_state_abs - LEG_MIN) / (LEG_MAX - LEG_MIN) * 9)\n\n    def state_monitor(self, monitoring_time = 10):\n        return reward_type\n\n    def opt_policy(self, state, next_action):   # optimal policy functio that chooses the action maximizing the reward\n\n    def transition_func(self, state, action):\n        return new_state\n\n    def RL_DT(self, RMax_, s_):   # execute action at given state\n        self.S_M.add(s_)        # adding state to set of states visited initially\n\n        while True: # end if s <- s'\n            # 1. get next action a from policy (?)\n            a_ = opt_policy(s_, a_current) #[hkh] its Utility func is determined by reward and transition func that are determined by DT\n\n            # 2. execute action a -> move_left, move_right or kick\n            if a_ == \"kick\": self.kick() elif a_ == \"move_left\": self.move_in() elif a_ == \"move_right\": self.move_out()\n            else: print(\"Nothing is given for optimal policy!\")\n            reward_type = self.state_monitor(monitoring_time = 10) # After taking an action, monitoring the state of the robot so that we can reward for that state-action pair.\n\n            # 3. Upon taking an action, receives reward\n            self.G_t += self.reward[reward_type] - self.reward[\"move_leg\"]  # According to the algorithm, punish with amount -2 as it's moved.\n            self.stateSet[s_][a_] += 1  # increase the state-action visits counter\n\n            # 4. reaches a new state s' <=> observe new state -> just read in leg angle again\n            s_new = transition_func(s_, a_)\n\n            # 5. check if new state has already been visited <=> check if it's in S_M\n            if not s_new in self.S_M:   # if not, add it to the stateSet in add_visited_states\n                self.S_M.add(s_new)\n                ## [hkh] new state action initializaiton is already done in constructor.\n\n            # 6. Update model -> many substeps\n            P_M, R_M, CH_ = self.update_model()    # taking function from Lennard part.\n\n            # 7. Check policy, if exploration/exploitation mode\n            exp_ = self.check_model\n\n            # 8. Compute values -> many substeps\n            if CH_:\n                self.compute_values(RMax_, P_M, R_M, self.S_M, exp_)\n        pass\n\n    # 1.\n    def get_action_policy(self):\n        pass\n\n    # 3.\n    def get_reward(self):   # Reward function: R(s,a)\n        # maybe with keyboard instead of tactile buttons -> 4 types of reward\n\n\n    # 6.\n    def add_visited_states(self):\n        for state in self.stateSet:\n            if state == self.leg_state_dis:\n                return\n        self.stateSet.append(self.leg_state_dis)\n\n    # 7. \n    def increase_visits(self):\n        pass\n\n    # 8.\n    def update_model(self):\n        pass\n    \n    # 9.\n    def check_policy(self):\n        pass\n\n    def compute_values(self):\n\n\n\n\n    # Callback function for reading in the joint values\n    def joints_cb(self, data):\n        self.joint_names = data.name  # LHipRoll for move in or move out\n        self.joint_angles = data.position\n        self.joint_velocities = data.velocity\n        # get leg state\n        for idx, names in enumerate(self.joint_names):\n            if names == \"LHipRoll\":\n                self.leg_state_abs = self.joint_angles[idx]\n                self.discretize_leg()\n                return\n    \n\n\n    # Read in the goal position!\n    # TODO: Aruco marker detection\n    def image_cb(self,data):\n        bridge_instance = CvBridge()\n\n    def set_joint_angles(self, head_angle, topic):\n        joint_angles_to_set = JointAnglesWithSpeed()\n        joint_angles_to_set.joint_names.append(topic) # each joint has a specific name, look into the joint_state topic or google  # When I\n        joint_angles_to_set.joint_angles.append(head_angle) # the joint values have to be in the same order as the names!!\n        joint_angles_to_set.relative = False # if true you can increment positions\n        joint_angles_to_set.speed = 0.08 # keep this low if you can\n        #print(str(joint_angles_to_set))\n        self.jointPub.publish(joint_angles_to_set)\n\n    def set_joint_angles_fast(self, head_angle, topic):\n        # fast motion!! careful\n        joint_angles_to_set = JointAnglesWithSpeed()\n        joint_angles_to_set.joint_names.append(topic) # each joint has a specific name, look into the joint_state topic or google  # When I\n        joint_angles_to_set.joint_angles.append(head_angle) # the joint values have to be in the same order as the names!!\n        joint_angles_to_set.relative = False # if true you can increment positions\n        joint_angles_to_set.speed = 0.4 # keep this low if you can\n        #print(str(joint_angles_to_set))\n        self.jointPub.publish(joint_angles_to_set)\n\n    def set_joint_angles_list(self, head_angle_list, topic_list):\n        # set the init one stand mode, doing it by all list together\n        if len(head_angle_list) == len(topic_list):\n            for i in range(len(topic_list)):\n                head_angle = head_angle_list[i]\n                topic = topic_list[i]\n                joint_angles_to_set = JointAnglesWithSpeed()\n                joint_angles_to_set.joint_names.append(topic) # each joint has a specific name, look into the joint_state topic or google  # When I\n                joint_angles_to_set.joint_angles.append(head_angle) # the joint values have to be in the same order as the names!!\n                joint_angles_to_set.relative = False # if true you can increment positions\n                joint_angles_to_set.speed = 0.1 # keep this low if you can\n                #print(str(joint_angles_to_set))\n                self.jointPub.publish(joint_angles_to_set)\n                rospy.sleep(0.05)\n\n    # Moves its left hip back and forward and then goes back into its initial position\n    def kick(self):\n        # i think\n\n        self.set_stiffness(True)\n        # Set ankle position to zero\n        #self.set_joint_angles(0.0, \"LAnklePitch\")\n        # Set hip roll position to zero\n        #self.set_joint_angles(0.0,\"LHipPitch\")\n        # Move foot back\n        self.set_joint_angles(0.48, \"LHipPitch\")\n        rospy.sleep(1.0)\n\n        self.set_joint_angles_fast(-1.1, \"LHipPitch\")\n        # Move the foot back after kick\n        rospy.sleep(2.0)\n        self.one_foot_stand()\n        #self.set_joint_angles(0.352, \"LHipPitch\")\n\n    def set_initial_stand(self):\n        robotIP = '10.152.246.137'\n        try:\n            postureProxy = ALProxy('ALRobotPosture', robotIP, 9559)\n        except Exception, e:\n            print('could not create ALRobotPosture')\n            print('Error was', e)\n        postureProxy.goToPosture('Stand', 1.0)\n        print(postureProxy.getPostureFamily())\n\n    def one_foot_stand(self):\n        # it is the init state ready for kicking\n        # careful !!!!!! very easy to fall\n        print('one foot mode')\n        self.set_stiffness(True)\n        # using the rostopic echo to get the desired joint states, not perfect\n        # rostopic echo /joint_states\n\n        # way1 easy to fall when kick\n        position = [0.004559993743896484, 0.5141273736953735, 1.8330880403518677, 0.19937801361083984, -1.9574260711669922,\n                    -1.5124820470809937, -0.8882279396057129, 0.32840001583099365, -0.13955211639404297, 0.31297802925109863,\n                    -0.3911280632019043, 1.4679961204528809, -0.8943638801574707, -0.12114405632019043, -0.13955211639404297,\n                    0.3697359561920166, 0.23772811889648438, -0.09232791513204575, 0.07980990409851074, -0.3282339572906494,\n                    1.676703929901123, -0.45717406272888184, 1.1964781284332275, 0.18872404098510742, 0.36965203285217285, 0.397599995136261]\n        '''\n        # way 2\n        \n        position = [-0.015382051467895508, 0.5120565295219421, 1.8346221446990967, 0.1779019832611084, -1.937483787536621,\n                    -1.5124820470809937, -1.0692400932312012, 0.32760000228881836, -0.11807608604431152, 0.31297802925109863,\n                    -0.3911280632019043, 1.4664621353149414, -0.8943638801574707, -0.12114405632019043, -0.11807608604431152,\n                    0.3697359561920166, 0.2530679702758789, -0.09232791513204575, -0.07665801048278809, -0.269942045211792,\n                    1.6951122283935547, -0.4617760181427002, 1.1949440240859985, 0.2025299072265625, 0.3589141368865967, 0.39399999380111694]\n        '''\n        # backup init position\n        # way3 [0.004559993743896484, 0.5141273736953735, 1.8330880403518677, 0.15335798263549805, -1.9129400253295898, -1.5032780170440674, -1.199629783630371, 0.32760000228881836, -0.22852396965026855, 0.4019498825073242, -0.3911280632019043, 1.4679961204528809, -0.8943638801574707, -0.12114405632019043, -0.22852396965026855, 0.3697359561920166, 0.23772811889648438, -0.09232791513204575, 0.08594608306884766, -0.3052239418029785, 1.6905097961425781, -0.44950389862060547, 1.1995460987091064, 0.1994619369506836, 0.3512439727783203, 0.3952000141143799]\n\n        # way2 not ok [-0.015382051467895508, 0.5120565295219421, 1.8346221446990967, 0.1779019832611084, -1.937483787536621, -1.5124820470809937, -1.0692400932312012, 0.32760000228881836, -0.11807608604431152, 0.31297802925109863, -0.3911280632019043, 1.4664621353149414, -0.8943638801574707, -0.12114405632019043, -0.11807608604431152, 0.3697359561920166, 0.2530679702758789, -0.09232791513204575, -0.07665801048278809, -0.269942045211792, 1.6951122283935547, -0.4617760181427002, 1.1949440240859985, 0.2025299072265625, 0.3589141368865967, 0.39399999380111694]\n        joints =['HeadYaw', 'HeadPitch', 'LShoulderPitch', 'LShoulderRoll', 'LElbowYaw', 'LElbowRoll', 'LWristYaw',\n                'LHand', 'LHipYawPitch', 'LHipRoll', 'LHipPitch', 'LKneePitch', 'LAnklePitch', 'LAnkleRoll', 'RHipYawPitch',\n                'RHipRoll', 'RHipPitch', 'RKneePitch', 'RAnklePitch', 'RAnkleRoll', 'RShoulderPitch', 'RShoulderRoll',\n                'RElbowYaw', 'RElbowRoll', 'RWristYaw', 'RHand']\n        self.set_joint_angles_list(position, joints)\n\n    def move_in(self):\n        print('move in')\n        LHipRoll_angle = self.joint_angles[self.joint_names.index('LHipRoll')]\n        print(self.joint_names.index('LHipRoll'), LHipRoll_angle)\n        if LHipRoll_angle > 0.45:\n            increment = -0.03\n            self.set_joint_angles(LHipRoll_angle + increment, \"LHipRoll\")\n        else:\n            print('low bound!!!')\n\n    def move_out(self):\n        print('move out')\n        LHipRoll_angle = self.joint_angles[self.joint_names.index('LHipRoll')]\n        #print(self.joint_names.index('LHipRoll'), LHipRoll_angle)\n        if LHipRoll_angle < 0.74:\n            increment = 0.03\n            self.set_joint_angles(LHipRoll_angle + increment, \"LHipRoll\")\n        else:\n            print('upper bound')\n\n    def set_initial_pos(self):\n        # fix the joints to the initial positions for the standing position\n        # I used the set_initial_stand instead - Tianle\n\n        # Head\n        self.set_stiffness(True)\n        self.set_joint_angles(-0.13503408432006836, \"HeadYaw\")\n        self.set_joint_angles(-0.49859189987182617, \"HeadPitch\")\n\n        # Right arm\n        self.set_joint_angles(-0.7439479827880859, \"RShoulderPitch\")\n        self.set_joint_angles(-0.019984006881713867, \"RShoulderRoll\")\n        self.set_joint_angles(0.30368995666503906, \"RElbowYaw\")\n        self.set_joint_angles(0.49245595932006836, \"RElbowRoll\")\n        self.set_joint_angles(1.2394300699234009, \"RWristYaw\")\n\n        # Left arm\n        self.set_joint_angles(1.8300200700759888, \"LShoulderPitch\")\n        self.set_joint_angles(0.12421202659606934, \"LShoulderRoll\")\n        self.set_joint_angles(-2.0856685638427734, \"LElbowYaw\")\n        self.set_joint_angles(-0.08125996589660645, \"LElbowRoll\")\n        self.set_joint_angles(-0.7424979209899902, \"LWristYaw\")\n\n        # Left leg & foot\n        self.set_joint_angles(-0.15642595291137695, \"LHipYawPitch\")\n        self.set_joint_angles(0.2224719524383545, \"LHipRoll\")\n        self.set_joint_angles(0.3528618812561035, \"LHipPitch\")\n        self.set_joint_angles(0.1, \"LKneePitch\")\n        self.set_joint_angles(-0.28996801376342773, \"LAnklePitch\")\n        self.set_joint_angles(-0.24079608917236328, \"LAnkleRoll\")\n\n        # Right leg & foot\n        self.set_joint_angles(-0.15642595291137695, \"RHipYawPitch\")\n        self.set_joint_angles(0.27616190910339355, \"RHipRoll\")\n        self.set_joint_angles(0.3742539882659912, \"RHipPitch\")\n        self.set_joint_angles(-0.09232791513204575, \"RKneePitch\")\n        self.set_joint_angles(-0.11961007118225098, \"RAnklePitch\")\n        self.set_joint_angles(-0.32050607681274414, \"RAnkleRoll\")       # That value is important for stability\n\n\n    def set_stiffness(self, value):\n        if value == True:\n            service_name = '/body_stiffness/enable'\n        elif value == False:\n            service_name = '/body_stiffness/disable'\n        try:\n            stiffness_service = rospy.ServiceProxy(service_name, Empty)\n            stiffness_service()\n        except rospy.ServiceException, e:\n            rospy.logerr(e)\n\n    '''\n    def touch_cb(self,data):\n        if data.button == 1 and data.state == 1:  # TB1\n            #self.set_stiffness(True)\n            print(\"Kick motion & stiffness enabled\")\n            self.kick()\n        if data.button == 2 and data.state == 1:\n            self.set_stiffness(False)\n            print(\"stiffness should be DISabled\")\n        if data.button == 3 and data.state == 1:\n            self.set_stiffness(True)\n            print(\"stiffness should be ENabled\")\n        # try kick motion\n        #if data.button == 3 and data.state == 1:\n        # left knee joint pitch: -0.092346 to 2.112528\n        # Left hip joint pitch: -1.535889 to 0.484090\n        # for RL motions the left hip roll is important: -0.379472 to 0.790477\n    '''\n\n    def touch_cb(self, data):\n        if data.button == 1 and data.state == 1:  # TB1\n            print(\"move in\")\n            self.move_in()\n        if data.button == 2 and data.state == 1:\n            print(\"move out\")\n            self.move_out()\n        if data.button == 3 and data.state == 1:\n            print(\"kick\")\n            self.kick()\n        # try kick motion\n        #if data.button == 3 and data.state == 1:\n        # left knee joint pitch: -0.092346 to 2.112528\n        # Left hip joint pitch: -1.535889 to 0.484090\n        # for RL motions the left hip roll is important: -0.379472 to 0.790477\n\n    def tutorial5_soccer_execute(self):\n\n        # cmac training here!!!\n        rospy.init_node('tutorial5_soccer_node', anonymous=True)\n        self.set_stiffness(True)\n        self.jointPub = rospy.Publisher(\"joint_angles\", JointAnglesWithSpeed, queue_size=10)\n\n        self.set_initial_stand()\n        rospy.sleep(2.0)\n        self.one_foot_stand()\n        #rospy.Subscriber(\"joint_states\",JointAnglesWithSpeed,self.joints_cb)\n        rospy.Subscriber(\"tactile_touch\", HeadTouch, self.touch_cb)\n        rospy.Subscriber('joint_states', JointState, self.joints_cb)\n        # start with setting the initial positions of head and right arm\n\n\n        #rospy.Subscriber(\"/nao_robot/camera/top/camera/image_raw\", Image, self.image_cb)\n\n        rospy.spin()\n\n\nclass RL_DT(tutorial5_soccer):\n    def __init__(self):\n        # Set of actions\n        self.actions = {\"move_right\": 0, \"move_left\": 1, \"kick\": 2}\n        self.state_action = {s: 0 for s in self.states}   # state-action pair is initially zero, no states visited so far\n        self.train_epsiodes = 100   # random guess\n        self.reward = {\"move_leg\": -1, \"fall\": -20, \"kick_fail\": -2, \"goal\": 20 }\n\n\n    def training(self):\n\n\n\nif __name__=='__main__':\n    ##[hkh]\n    MDP(init = init, actlist= action_list, terminals= terminal_state, states=states, transitions=transition_model)\n\n    node_instance = tutorial5_soccer()\n    #node_instance.one_foot_stand()\n    node_instance.tutorial5_soccer_execute()\n    #node_instance.stand()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/tutorial_5/scripts/tutorial5_soccer.py b/src/tutorial_5/scripts/tutorial5_soccer.py
--- a/src/tutorial_5/scripts/tutorial5_soccer.py	(revision eefb2aed87f3476bad0ddd0a8db0a6e090c77324)
+++ b/src/tutorial_5/scripts/tutorial5_soccer.py	(date 1657573171971)
@@ -81,10 +81,10 @@
                 ## [hkh] new state action initializaiton is already done in constructor.
 
             # 6. Update model -> many substeps
-            P_M, R_M, CH_ = self.update_model()    # taking function from Lennard part.
+            P_M, R_M, CH_ = self.update_model()
 
-            # 7. Check policy, if exploration/exploitation mode
-            exp_ = self.check_model
+            # 7. Check model, if exploration/exploitation mode
+            exp_, CH_ = self.check_model(P_M, R_M)
 
             # 8. Compute values -> many substeps
             if CH_:
@@ -116,10 +116,47 @@
         pass
     
     # 9.
-    def check_policy(self):
-        pass
+    def check_model(self, P_M, R_M):
+        return exploration, change_model
+
+    def compute_values(self, RMax_, P_M, R_M, self.S_M, exp_):   # from Lennard code 11.07.22
+        # Initialize all state's step counts
+        self.steps_nearest_visited_state = {x: sys.maxint for x in range(9)}
+        visits_values = []
+        for state in self.stateSet:
+            visits_value = self.get_visits_state(state)
+            visits_values.append(visits_value)
+            if visits_value > 0:
+                self.steps_nearest_visited_state[state] = 0
+        min_visits = min(visits_values)
 
-    def compute_values(self):
+        # Perform value iteration on the model
+        action_values_temp = {}
+        converged = False
+        while not converged:
+            for state in self.stateSet:
+                for action in self.actions:
+                    if self.exploration and visits_value(state) == min_visits:
+                        # Unknown states are given exploration bonus
+                        action_values_temp[(state, action)] = RMax
+                    elif self.steps_nearest_visited_state[state] > MAX_STEPS:
+                        action_values_temp[(state, action)] = RMax
+                    else:
+                        # Update remaining state's action values
+                        action_values_temp[(state, action)] = self.get_reward_state_action_pair(state, action)
+                        for next_state in self.get_next_states(state):
+                            if next_state not in self.stateSet:
+                                self.stateSet.append(next_state)
+                                for next_action in self.actions:
+                                    self.visits[(next_state, next_action)] = 0
+                            # Update action-values using Bellman Equation
+                            action_values_temp[(state, action)] += \
+                                DISCOUNT_FACTOR * \
+                                self.get_prop_next_state_given_state_action(next_state, state, action) * \
+                                self.get_next_state_action_value_greedy(next_state)
+            converged = self.check_convergence(action_values_temp)
+            self.action_values = action_values_temp
+
 
 
 
