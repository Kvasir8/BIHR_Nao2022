{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NormalizeTransform:\n",
    "    \"\"\"\n",
    "    Transform class to normalize images using mean and std\n",
    "    Functionality depends on the mean and std provided in __init__():\n",
    "        - if mean and std are single values, normalize the entire image\n",
    "        - if mean and std are numpy arrays of size C for C image channels,\n",
    "            then normalize each image channel separately\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        \"\"\"\n",
    "        :param mean: mean of images to be normalized\n",
    "            can be a single value, or a numpy array of size C\n",
    "        :param std: standard deviation of images to be normalized\n",
    "             can be a single value or a numpy array of size C\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, images):\n",
    "        images = (images - self.mean) / self.std\n",
    "        return images\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ClassificationNet(Network):\n",
    "    \"\"\"\n",
    "    A fully-connected classification neural network with configurable\n",
    "    activation function, number of layers, number of classes, hidden size and\n",
    "    regularization strength.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation=Sigmoid(), num_layer=2,\n",
    "                 input_size=3 * 32 * 32, hidden_size=100,\n",
    "                 std=1e-3, num_classes=10, reg=0, **kwargs):\n",
    "        \"\"\"\n",
    "        :param activation: choice of activation function. It should implement\n",
    "            a forward() and a backward() method.\n",
    "        :param num_layer: integer, number of layers.\n",
    "        :param input_size: integer, the dimension D of the input data.\n",
    "        :param hidden_size: integer, the number of neurons H in the hidden layer.\n",
    "        :param std: float, standard deviation used for weight initialization.\n",
    "        :param num_classes: integer, number of classes.\n",
    "        :param reg: float, regularization strength.\n",
    "        \"\"\"\n",
    "        super(ClassificationNet, self).__init__(\"cifar10_classification_net\")\n",
    "\n",
    "        self.activation = activation\n",
    "        self.reg_strength = reg\n",
    "\n",
    "        self.cache = None\n",
    "\n",
    "        self.memory = 0\n",
    "        self.memory_forward = 0\n",
    "        self.memory_backward = 0\n",
    "        self.num_operation = 0\n",
    "\n",
    "        # Initialize random gaussian weights for all layers and zero bias\n",
    "        self.num_layer = num_layer\n",
    "        self.params = {'W1': std * np.random.randn(input_size, hidden_size),\n",
    "                       'b1': np.zeros(hidden_size)}\n",
    "\n",
    "        for i in range(num_layer - 2):\n",
    "            self.params['W' + str(i + 2)] = std * np.random.randn(hidden_size,\n",
    "                                                                  hidden_size)\n",
    "            self.params['b' + str(i + 2)] = np.zeros(hidden_size)\n",
    "\n",
    "        self.params['W' + str(num_layer)] = std * np.random.randn(hidden_size,\n",
    "                                                                  num_classes)\n",
    "        self.params['b' + str(num_layer)] = np.zeros(num_classes)\n",
    "\n",
    "        self.grads = {}\n",
    "        self.reg = {}\n",
    "        for i in range(num_layer):\n",
    "            self.grads['W' + str(i + 1)] = 0.0\n",
    "            self.grads['b' + str(i + 1)] = 0.0\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the model.\n",
    "\n",
    "        :param X: Input data of shape N x D. Each X[i] is a training sample.\n",
    "        :return: Predicted value for the data in X, shape N x 1\n",
    "                 1-dimensional array of length N with the classification scores.\n",
    "        \"\"\"\n",
    "\n",
    "        self.cache = {}\n",
    "        self.reg = {}\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        # Unpack variables from the params dictionary\n",
    "        for i in range(self.num_layer - 1):\n",
    "            W, b = self.params['W' + str(i + 1)], self.params['b' + str(i + 1)]\n",
    "\n",
    "            # Forward i_th layer\n",
    "            X, cache_affine = affine_forward(X, W, b)\n",
    "            self.cache[\"affine\" + str(i + 1)] = cache_affine\n",
    "\n",
    "            # Activation function\n",
    "            X, cache_sigmoid = self.activation.forward(X)\n",
    "            self.cache[\"sigmoid\" + str(i + 1)] = cache_sigmoid\n",
    "\n",
    "            # Store the reg for the current W\n",
    "            self.reg['W' + str(i + 1)] = np.sum(W ** 2) * self.reg_strength\n",
    "\n",
    "        # last layer contains no activation functions\n",
    "        W, b = self.params['W' + str(self.num_layer)],\\\n",
    "               self.params['b' + str(self.num_layer)]\n",
    "        y, cache_affine = affine_forward(X, W, b)\n",
    "        self.cache[\"affine\" + str(self.num_layer)] = cache_affine\n",
    "        self.reg['W' + str(self.num_layer)] = np.sum(W ** 2) * self.reg_strength\n",
    "\n",
    "        return y\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Performs the backward pass of the model.\n",
    "\n",
    "        :param dy: N x 1 array. The gradient wrt the output of the network.\n",
    "        :return: Gradients of the model output wrt the model weights\n",
    "        \"\"\"\n",
    "\n",
    "        # Note that last layer has no activation\n",
    "        cache_affine = self.cache['affine' + str(self.num_layer)]\n",
    "        dh, dW, db = affine_backward(dy, cache_affine)\n",
    "        self.grads['W' + str(self.num_layer)] = \\\n",
    "            dW + 2 * self.reg_strength * self.params['W' + str(self.num_layer)]\n",
    "        self.grads['b' + str(self.num_layer)] = db\n",
    "\n",
    "        # The rest sandwich layers\n",
    "        for i in range(self.num_layer - 2, -1, -1):\n",
    "            # Unpack cache\n",
    "            cache_sigmoid = self.cache['sigmoid' + str(i + 1)]\n",
    "            cache_affine = self.cache['affine' + str(i + 1)]\n",
    "\n",
    "            # Activation backward\n",
    "            dh = self.activation.backward(dh, cache_sigmoid)\n",
    "\n",
    "            # Affine backward\n",
    "            dh, dW, db = affine_backward(dh, cache_affine)\n",
    "\n",
    "            # Refresh the gradients\n",
    "            self.grads['W' + str(i + 1)] = dW + 2 * self.reg_strength * \\\n",
    "                                           self.params['W' + str(i + 1)]\n",
    "            self.grads['b' + str(i + 1)] = db\n",
    "\n",
    "        return self.grads\n",
    "\n",
    "    def save_model(self):\n",
    "        directory = 'models'\n",
    "        model = {self.model_name: self}\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        pickle.dump(model, open(directory + '/' + self.model_name + '.p', 'wb'))\n",
    "\n",
    "    def get_dataset_prediction(self, loader):\n",
    "        scores = []\n",
    "        labels = []\n",
    "\n",
    "        for batch in loader:\n",
    "            X = batch['image']\n",
    "            y = batch['label']\n",
    "            score = self.forward(X)\n",
    "            scores.append(score)\n",
    "            labels.append(y)\n",
    "\n",
    "        scores = np.concatenate(scores, axis=0)\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "        preds = scores.argmax(axis=1)\n",
    "        acc = (labels == preds).mean()\n",
    "\n",
    "        return labels, preds, acc\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    :param x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    :param w: A numpy array of weights, of shape (D, M)\n",
    "    :param b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    :return out: output, of shape (N, M)\n",
    "    :return cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    x_reshaped = np.reshape(x, (x.shape[0], -1))\n",
    "    out = x_reshaped.dot(w) + b\n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    :param dout: Upstream derivative, of shape (N, M)\n",
    "    :param cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "      - b: A numpy array of biases, of shape (M,\n",
    "\n",
    "    :return dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    :return dw: Gradient with respect to w, of shape (D, M)\n",
    "    :return db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    dw = np.reshape(x, (x.shape[0], -1)).T.dot(dout)\n",
    "    dw = np.reshape(dw, w.shape)\n",
    "\n",
    "    db = np.sum(dout, axis=0, keepdims=False)\n",
    "\n",
    "    dx = dout.dot(w.T)\n",
    "    dx = np.reshape(dx, x.shape)\n",
    "    return dx, dw, db\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Inputs, of any shape\n",
    "\n",
    "        :return out: Output, of the same shape as x\n",
    "        :return cache: Cache, for backward computation, of the same shape as x\n",
    "        \"\"\"\n",
    "        outputs = 1 / (1 + np.exp(-x))\n",
    "        cache = outputs\n",
    "        return outputs, cache\n",
    "\n",
    "    def backward(self, dout, cache):\n",
    "        \"\"\"\n",
    "        :return: dx: the gradient w.r.t. input X, of the same shape as X\n",
    "        \"\"\"\n",
    "        dx = None\n",
    "        dx = dout * cache * (1 - cache)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Inputs, of any shape\n",
    "\n",
    "        :return out: Output, of the same shape as x\n",
    "        :return cache: Cache, for backward computation, of the same shape as x\n",
    "        \"\"\"\n",
    "        outputs = None\n",
    "        cache = None\n",
    "        ########################################################################\n",
    "        # TODO:                                                                #\n",
    "        # Implement the forward pass of Relu activation function               #\n",
    "        ########################################################################\n",
    "\n",
    "        pass\n",
    "\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        return outputs, cache\n",
    "\n",
    "    def backward(self, dout, cache):\n",
    "        \"\"\"\n",
    "        :return: dx: the gradient w.r.t. input X, of the same shape as X\n",
    "        \"\"\"\n",
    "        dx = None\n",
    "        ########################################################################\n",
    "        # TODO:                                                                #\n",
    "        # Implement the backward pass of Relu activation function              #\n",
    "        ########################################################################\n",
    "\n",
    "        pass\n",
    "\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        return dx\n",
    "\n",
    "\n",
    "class LeakyRelu:\n",
    "    def __init__(self, slope=0.01):\n",
    "        self.slope = slope\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Inputs, of any shape\n",
    "\n",
    "        :return out: Output, of the same shape as x\n",
    "        :return cache: Cache, for backward computation, of the same shape as x\n",
    "        \"\"\"\n",
    "        outputs = None\n",
    "        cache = None\n",
    "        ########################################################################\n",
    "        # TODO:                                                                #\n",
    "        # Implement the forward pass of LeakyRelu activation function          #\n",
    "        ########################################################################\n",
    "\n",
    "        pass\n",
    "\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        return outputs, cache\n",
    "\n",
    "    def backward(self, dout, cache):\n",
    "        \"\"\"\n",
    "        :return: dx: the gradient w.r.t. input X, of the same shape as X\n",
    "        \"\"\"\n",
    "        dx = None\n",
    "        ########################################################################\n",
    "        # TODO:                                                                #\n",
    "        # Implement the backward pass of LeakyRelu activation function         #\n",
    "        ########################################################################\n",
    "\n",
    "        pass\n",
    "\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Inputs, of any shape\n",
    "\n",
    "        :return out: Output, of the same shape as x\n",
    "        :return cache: Cache, for backward computation, of the same shape as x\n",
    "        \"\"\"\n",
    "        outputs = None\n",
    "        cache = None\n",
    "        ########################################################################\n",
    "        # TODO:                                                                #\n",
    "        # Implement the forward pass of Tanh activation function               #\n",
    "        ########################################################################\n",
    "\n",
    "        pass\n",
    "\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        return outputs, cache\n",
    "\n",
    "    def backward(self, dout, cache):\n",
    "        \"\"\"\n",
    "        :return: dx: the gradient w.r.t. input X, of the same shape as X\n",
    "        \"\"\"\n",
    "        dx = None\n",
    "        ########################################################################\n",
    "        # TODO:                                                                #\n",
    "        # Implement the backward pass of Tanh activation function              #\n",
    "        ########################################################################\n",
    "\n",
    "        pass\n",
    "\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        return dx\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CrossEntropyFromLogits(Loss):\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, y_out, y_truth, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the cross entropy loss function.\n",
    "\n",
    "        :param y_out: [N, C] array with the predicted logits of the model\n",
    "            (i.e. the value before applying any activation)\n",
    "        :param y_truth: [N, ] array with ground truth labels.\n",
    "\n",
    "        :return: float, the cross-entropy loss value\n",
    "        \"\"\"\n",
    "\n",
    "        # Transform the ground truth labels into one hot encodings.\n",
    "        N, C = y_out.shape\n",
    "        y_truth_one_hot = np.zeros_like(y_out)\n",
    "        y_truth_one_hot[np.arange(N), y_truth] = 1\n",
    "\n",
    "        # Transform the logits into a distribution using softmax.\n",
    "        y_out_exp = np.exp(y_out - np.max(y_out, axis=1, keepdims=True))\n",
    "        y_out_probs = y_out_exp / np.sum(y_out_exp, axis=1, keepdims=True)\n",
    "\n",
    "        # Compute the loss for each element in the batch.\n",
    "        loss = -y_truth_one_hot * np.log(y_out_probs)\n",
    "        loss = loss.sum(axis=1).mean()\n",
    "\n",
    "        self.cache['probs'] = y_out_probs\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, y_out, y_truth):\n",
    "        N, C = y_out.shape\n",
    "        gradient = self.cache['probs']\n",
    "        gradient[np.arange(N), y_truth] -= 1\n",
    "        gradient /= N\n",
    "\n",
    "        return gradient\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}