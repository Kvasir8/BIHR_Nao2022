{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Some lengthy setup.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "from exercise_code.networks.layer import (\n",
    "    Sigmoid,\n",
    "    Relu,\n",
    "    LeakyRelu,\n",
    "    Tanh,\n",
    ")\n",
    "from exercise_code.data import (\n",
    "    DataLoader,\n",
    "    ImageFolderDataset,\n",
    "    MemoryImageFolderDataset,\n",
    "    RescaleTransform,\n",
    "    NormalizeTransform,\n",
    "    FlattenTransform,\n",
    "    ComposeTransform,\n",
    ")\n",
    "from exercise_code.data.image_folder_dataset import RandomHorizontalFlip\n",
    "from exercise_code.networks import (\n",
    "    ClassificationNet,\n",
    "    BCE,\n",
    "    CrossEntropyFromLogits\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "download_url = \"https://i2dl.dvl.in.tum.de/downloads/cifar10.zip\"\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "cifar_root = os.path.join(i2dl_exercises_path, \"datasets\", \"cifar10\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATASET = ImageFolderDataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use the Cifar10 mean and standard deviation computed in Exercise 3.\n",
    "cifar_mean = np.array([0.49191375, 0.48235852, 0.44673872])\n",
    "cifar_std  = np.array([0.24706447, 0.24346213, 0.26147554])\n",
    "\n",
    "# Define all the transforms we will apply on the images when\n",
    "# retrieving them.\n",
    "rescale_transform = RescaleTransform()\n",
    "normalize_transform = NormalizeTransform(\n",
    "    mean=cifar_mean,\n",
    "    std=cifar_std\n",
    ")\n",
    "flatten_transform = FlattenTransform()\n",
    "compose_transform = ComposeTransform([rescale_transform,\n",
    "                                      normalize_transform,\n",
    "                                      flatten_transform])\n",
    "\n",
    "# Create a train, validation and test dataset.\n",
    "datasets = {}\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    crt_dataset = DATASET(\n",
    "        mode=mode,\n",
    "        root=cifar_root,\n",
    "        transform=compose_transform,\n",
    "        split={'train': 0.6, 'val': 0.2, 'test': 0.2}\n",
    "    )\n",
    "    datasets[mode] = crt_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dataloader for each split.\n",
    "dataloaders = {}\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    crt_dataloader = DataLoader(\n",
    "        dataset=datasets[mode],\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    dataloaders[mode] = crt_dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_size = datasets['train'][0]['image'].shape[0]\n",
    "model = ClassificationNet(input_size=input_size,\n",
    "                          hidden_size=512)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_layer = 2\n",
    "reg = 0.1\n",
    "\n",
    "model = ClassificationNet(activation=Sigmoid(),\n",
    "                          num_layer=num_layer,\n",
    "                          reg=reg,\n",
    "                          num_classes=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 1.4 Loss\n",
    "class MSE(Loss):\n",
    "\n",
    "    def forward(self, y_out, y_truth, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the MSE loss function.\n",
    "\n",
    "        :param y_out: [N, ] array predicted value of your model.\n",
    "                y_truth: [N, ] array ground truth value of your training set.\n",
    "        :return: [N, ] array of MSE loss for each sample of your training set.\n",
    "        \"\"\"\n",
    "\n",
    "        result = (y_out - y_truth)**2\n",
    "\n",
    "        if reduction == 'mean':\n",
    "            result = result.mean()\n",
    "        elif reduction == 'sum':\n",
    "            result = result.sum()\n",
    "        elif reduction == 'none':\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return result\n",
    "\n",
    "    def backward(self, y_out, y_truth):\n",
    "        \"\"\"\n",
    "        Performs the backward pass of the MSE loss function.\n",
    "\n",
    "        :param y_out: [N, ] array predicted value of your model.\n",
    "               y_truth: [N, ] array ground truth value of your training set.\n",
    "        :return: [N, ] array of MSE loss gradients w.r.t y_out for\n",
    "                  each sample of your training set.\n",
    "        \"\"\"\n",
    "\n",
    "        gradient = 2 * (y_out - y_truth)\n",
    "\n",
    "        return gradient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 1.5 Optimizer\n",
    "for param in model:\n",
    "    # Use the gradient to update the weights.\n",
    "    update(param)\n",
    "\n",
    "    # Reset the gradient after each update.\n",
    "    param.gradient = 0\n",
    "\n",
    "SGD had the simplest update rule:\n",
    "\n",
    "def update(param):\n",
    "    param = param - learning_rate * param.gradient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 1.6 Solver: performs GD using given learning rate\n",
    "solver = Solver(model,\n",
    "                dataloaders['train'],\n",
    "                dataloaders['val'],\n",
    "                learning_rate=0.001,\n",
    "                loss_func=MSE(),\n",
    "                optimizer=SGD)\n",
    "\n",
    "solver.train(epochs=epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    \"\"\"\n",
    "    A Solver encapsulates all the logic necessary for training classification\n",
    "    or regression models.\n",
    "    The Solver performs gradient descent using the given learning rate.\n",
    "\n",
    "    The solver accepts both training and validataion data and labels so it can\n",
    "    periodically check classification accuracy on both training and validation\n",
    "    data to watch out for overfitting.\n",
    "\n",
    "    To train a model, you will first construct a Solver instance, passing the\n",
    "    model, dataset, learning_rate to the constructor.\n",
    "    You will then call the train() method to run the optimization\n",
    "    procedure and train the model.\n",
    "\n",
    "    After the train() method returns, model.params will contain the parameters\n",
    "    that performed best on the validation set over the course of training.\n",
    "    In addition, the instance variable solver.loss_history will contain a list\n",
    "    of all losses encountered during training and the instance variables\n",
    "    solver.train_loss_history and solver.val_loss_history will be lists\n",
    "    containing the losses of the model on the training and validation set at\n",
    "    each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, train_dataloader, val_dataloader,\n",
    "                 loss_func=CrossEntropyFromLogits(), learning_rate=1e-3,\n",
    "                 optimizer=Adam, verbose=True, print_every=1, lr_decay = 1.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Construct a new Solver instance.\n",
    "\n",
    "        Required arguments:\n",
    "        - model: A model object conforming to the API described above\n",
    "\n",
    "        - train_dataloader: A generator object returning training data\n",
    "        - val_dataloader: A generator object returning validation data\n",
    "\n",
    "        - loss_func: Loss function object.\n",
    "        - learning_rate: Float, learning rate used for gradient descent.\n",
    "\n",
    "        - optimizer: The optimizer specifying the update rule\n",
    "\n",
    "        Optional arguments:\n",
    "        - verbose: Boolean; if set to false then no output will be printed during\n",
    "          training.\n",
    "        - print_every: Integer; training losses will be printed every print_every\n",
    "          iterations.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "        self.opt = optimizer(model, loss_func, learning_rate)\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.print_every = print_every\n",
    "\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "\n",
    "        self.current_patience = 0\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Set up some book-keeping variables for optimization. Don't call this\n",
    "        manually.\n",
    "        \"\"\"\n",
    "        # Set up some variables for book-keeping\n",
    "        self.best_model_stats = None\n",
    "        self.best_params = None\n",
    "\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = []\n",
    "\n",
    "        self.train_batch_loss = []\n",
    "        self.val_batch_loss = []\n",
    "\n",
    "        self.num_operation = 0\n",
    "        self.current_patience = 0\n",
    "\n",
    "    def _step(self, X, y, validation=False):\n",
    "        \"\"\"\n",
    "        Make a single gradient update. This is called by train() and should not\n",
    "        be called manually.\n",
    "\n",
    "        :param X: batch of training features\n",
    "        :param y: batch of corresponding training labels\n",
    "        :param validation: Boolean indicating whether this is a training or\n",
    "            validation step\n",
    "\n",
    "        :return loss: Loss between the model prediction for X and the target\n",
    "            labels y\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = self.model.forward(X)\n",
    "        # Compute loss\n",
    "        loss = self.loss_func.forward(y_pred, y)\n",
    "        # Add the regularization\n",
    "        loss += sum(self.model.reg.values())\n",
    "\n",
    "        # Count number of operations\n",
    "        self.num_operation += self.model.num_operation\n",
    "\n",
    "        # Perform gradient update (only in train mode)\n",
    "        if not validation:\n",
    "            # Compute gradients\n",
    "            self.opt.backward(y_pred, y)\n",
    "            # Update weights\n",
    "            self.opt.step()\n",
    "\n",
    "            # If it was a training step, we need to count operations for\n",
    "            # backpropagation as well\n",
    "            self.num_operation += self.model.num_operation\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self, epochs=100, patience = None):\n",
    "        \"\"\"\n",
    "        Run optimization to train the model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Start an epoch\n",
    "        for t in range(epochs):\n",
    "\n",
    "            # Iterate over all training samples\n",
    "            train_epoch_loss = 0.0\n",
    "\n",
    "            for batch in self.train_dataloader:\n",
    "                # Unpack data\n",
    "                X = batch['image']\n",
    "                y = batch['label']\n",
    "\n",
    "                # Update the model parameters.\n",
    "                validate = t == 0\n",
    "                train_loss = self._step(X, y, validation=validate)\n",
    "\n",
    "                self.train_batch_loss.append(train_loss)\n",
    "                train_epoch_loss += train_loss\n",
    "\n",
    "            train_epoch_loss /= len(self.train_dataloader)\n",
    "\n",
    "\n",
    "            self.opt.lr *= self.lr_decay\n",
    "\n",
    "\n",
    "            # Iterate over all validation samples\n",
    "            val_epoch_loss = 0.0\n",
    "\n",
    "            for batch in self.val_dataloader:\n",
    "                # Unpack data\n",
    "                X = batch['image']\n",
    "                y = batch['label']\n",
    "\n",
    "                # Compute Loss - no param update at validation time!\n",
    "                val_loss = self._step(X, y, validation=True)\n",
    "                self.val_batch_loss.append(val_loss)\n",
    "                val_epoch_loss += val_loss\n",
    "\n",
    "            val_epoch_loss /= len(self.val_dataloader)\n",
    "\n",
    "            # Record the losses for later inspection.\n",
    "            self.train_loss_history.append(train_epoch_loss)\n",
    "            self.val_loss_history.append(val_epoch_loss)\n",
    "\n",
    "            if self.verbose and t % self.print_every == 0:\n",
    "                print('(Epoch %d / %d) train loss: %f; val loss: %f' % (\n",
    "                    t + 1, epochs, train_epoch_loss, val_epoch_loss))\n",
    "\n",
    "            # Keep track of the best model\n",
    "            self.update_best_loss(val_epoch_loss, train_epoch_loss)\n",
    "            if patience and self.current_patience >= patience:\n",
    "                print(\"Stopping early at epoch {}!\".format(t))\n",
    "                break\n",
    "\n",
    "        # At the end of training swap the best params into the model\n",
    "        self.model.params = self.best_params\n",
    "\n",
    "    def get_dataset_accuracy(self, loader):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in loader:\n",
    "            X = batch['image']\n",
    "            y = batch['label']\n",
    "            y_pred = self.model.forward(X)\n",
    "            label_pred = np.argmax(y_pred, axis=1)\n",
    "            correct += sum(label_pred == y)\n",
    "            if y.shape:\n",
    "                total += y.shape[0]\n",
    "            else:\n",
    "                total += 1\n",
    "        return correct / total\n",
    "\n",
    "    def update_best_loss(self, val_loss, train_loss):\n",
    "        # Update the model and best loss if we see improvements.\n",
    "        if not self.best_model_stats or val_loss < self.best_model_stats[\"val_loss\"]:\n",
    "            self.best_model_stats = {\"val_loss\":val_loss, \"train_loss\":train_loss}\n",
    "            self.best_params = self.model.params\n",
    "            self.current_patience = 0\n",
    "        else:\n",
    "            self.current_patience += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}