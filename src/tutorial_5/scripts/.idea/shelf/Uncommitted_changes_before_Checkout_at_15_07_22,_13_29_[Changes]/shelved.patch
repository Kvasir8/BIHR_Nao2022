Index: tutorial5_soccer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/usr/bin/env python\nimport random\nimport rospy\nfrom std_msgs.msg import String, Header\nfrom std_srvs.srv import Empty\nfrom naoqi_bridge_msgs.msg import JointAnglesWithSpeed,Bumper,HeadTouch\nfrom naoqi import ALProxy\nfrom sensor_msgs.msg import Image,JointState\nfrom cv_bridge import CvBridge, CvBridgeError\nimport cv2\nimport numpy as np\nimport csv\nimport random\nfrom naoqi import ALProxy\nimport sys\nfrom algorithm_module import algorithm\n\nLEG_MAX = 0.790477\nLEG_MIN = -0.379472\n\nSTATES = 10\nEPISODES = 200\n\nclass tutorial5_soccer:\n    def __init__(self):\n        self.blobX = 0\n        self.blobY = 0\n        self.blobSize = 0\n        self.shoulderRoll = 0\n        self.shoulderPitch = 0\n        # For setting the stiffnes of single joints\n        self.jointPub = 0\n        self.leg_state_dis = 10         # this variable must hold the current discrete state!!\n        self.leg_state_abs = 0\n        self.rldt = algorithm(STATES)\n\n    # Callback function for reading in the joint values\n    def joints_cb(self, data):\n        #rospy.loginfo(\"joint states \"+str(data.name)+str(data.position))\n        # store current joint information in class variables\n        self.joint_names = data.name  # LHipRoll for move in or move out\n        self.joint_angles = data.position\n        self.joint_velocities = data.velocity\n        for idx, names in enumerate(self.joint_names):\n            if names == \"LHipRoll\":\n                self.leg_state_abs = self.joint_angles[idx]\n                self.discretize_leg()\n        \n        #print(\"Current leg state: \", self.leg_state_dis)\n        \n    def discretize_leg(self):\n        \n        tmp = np.round((self.leg_state_abs - LEG_MIN) / (LEG_MAX - LEG_MIN) * 9.0)\n       \n        self.leg_state_dis = tmp\n\n\n    # Read in the goal position!\n    # TODO: Aruco marker detection\n    def image_cb(self,data):\n        bridge_instance = CvBridge()\n\n\n\n    # TODO: put in cmac logic!\n    # input to cmac mapping: self.blobX, self.blobY\n    def move_arm(self):\n        # for testing - random arm values\n\n        self.set_joint_angles(self.shoulderPitch, \"RShoulderPitch\")\n        self.set_joint_angles(self.shoulderRoll, \"RShoulderRoll\")\n\n\n\n    def set_joint_angles(self, head_angle, topic):\n        joint_angles_to_set = JointAnglesWithSpeed()\n        joint_angles_to_set.joint_names.append(topic) # each joint has a specific name, look into the joint_state topic or google  # When I\n        joint_angles_to_set.joint_angles.append(head_angle) # the joint values have to be in the same order as the names!!\n        joint_angles_to_set.relative = False # if true you can increment positions\n        joint_angles_to_set.speed = 0.08 # keep this low if you can\n        #print(str(joint_angles_to_set))\n        self.jointPub.publish(joint_angles_to_set)\n\n    def set_joint_angles_fast(self, head_angle, topic):\n        # fast motion!! careful\n        joint_angles_to_set = JointAnglesWithSpeed()\n        joint_angles_to_set.joint_names.append(topic) # each joint has a specific name, look into the joint_state topic or google  # When I\n        joint_angles_to_set.joint_angles.append(head_angle) # the joint values have to be in the same order as the names!!\n        joint_angles_to_set.relative = False # if true you can increment positions\n        joint_angles_to_set.speed = 0.2 # keep this low if you can\n        #print(str(joint_angles_to_set))\n        self.jointPub.publish(joint_angles_to_set)\n\n    def set_joint_angles_list(self, head_angle_list, topic_list):\n        # set the init one stand mode, doing it by all list together\n        if len(head_angle_list) == len(topic_list):\n            for i in range(len(topic_list)):\n                head_angle = head_angle_list[i]\n                topic = topic_list[i]\n                joint_angles_to_set = JointAnglesWithSpeed()\n                joint_angles_to_set.joint_names.append(topic) # each joint has a specific name, look into the joint_state topic or google  # When I\n                joint_angles_to_set.joint_angles.append(head_angle) # the joint values have to be in the same order as the names!!\n                joint_angles_to_set.relative = False # if true you can increment positions\n                joint_angles_to_set.speed = 0.1 # keep this low if you can\n                #print(str(joint_angles_to_set))\n                self.jointPub.publish(joint_angles_to_set)\n                rospy.sleep(0.05)\n\n    # Moves its left hip back and forward and then goes back into its initial position\n    def kick(self):\n        # i think\n\n        self.set_stiffness(True)\n      \n        self.set_joint_angles(0.48, \"LHipPitch\")\n        rospy.sleep(1.0)\n\n        self.set_joint_angles_fast(-1.1, \"LHipPitch\")\n        # Move the foot back after kick\n        rospy.sleep(2.0)\n        self.one_foot_stand()\n        #self.set_joint_angles(0.352, \"LHipPitch\")\n\n    def set_initial_stand(self):\n        robotIP = '10.152.246.137'\n        try:\n            postureProxy = ALProxy('ALRobotPosture', robotIP, 9559)\n        except Exception, e:\n            print('could not create ALRobotPosture')\n            print('Error was', e)\n        postureProxy.goToPosture('Stand', 1.0)\n        print(postureProxy.getPostureFamily())\n\n    def one_foot_stand(self):\n        # it is the init state ready for kicking\n        # careful !!!!!! very easy to fall\n        print('one foot mode')\n        self.set_stiffness(True)\n        # using the rostopic echo to get the desired joint states, not perfect\n        # rostopic echo /joint_states\n\n        # way1 easy to fall when kick\n        position = [0.004559993743896484, 0.5141273736953735, 1.8330880403518677, 0.19937801361083984, -1.9574260711669922,\n                    -1.5124820470809937, -0.8882279396057129, 0.32840001583099365, -0.13955211639404297, 0.31297802925109863,\n                    -0.3911280632019043, 1.4679961204528809, -0.8943638801574707, -0.12114405632019043, -0.13955211639404297,\n                    0.3697359561920166, 0.23772811889648438, -0.09232791513204575, 0.07980990409851074, -0.3282339572906494,\n                    1.676703929901123, -0.45717406272888184, 1.1964781284332275, 0.18872404098510742, 0.36965203285217285, 0.397599995136261]\n        '''\n        # way 2\n        \n        position = [-0.015382051467895508, 0.5120565295219421, 1.8346221446990967, 0.1779019832611084, -1.937483787536621,\n                    -1.5124820470809937, -1.0692400932312012, 0.32760000228881836, -0.11807608604431152, 0.31297802925109863,\n                    -0.3911280632019043, 1.4664621353149414, -0.8943638801574707, -0.12114405632019043, -0.11807608604431152,\n                    0.3697359561920166, 0.2530679702758789, -0.09232791513204575, -0.07665801048278809, -0.269942045211792,\n                    1.6951122283935547, -0.4617760181427002, 1.1949440240859985, 0.2025299072265625, 0.3589141368865967, 0.39399999380111694]\n        '''\n        # backup init position\n        # way3 [0.004559993743896484, 0.5141273736953735, 1.8330880403518677, 0.15335798263549805, -1.9129400253295898, -1.5032780170440674, -1.199629783630371, 0.32760000228881836, -0.22852396965026855, 0.4019498825073242, -0.3911280632019043, 1.4679961204528809, -0.8943638801574707, -0.12114405632019043, -0.22852396965026855, 0.3697359561920166, 0.23772811889648438, -0.09232791513204575, 0.08594608306884766, -0.3052239418029785, 1.6905097961425781, -0.44950389862060547, 1.1995460987091064, 0.1994619369506836, 0.3512439727783203, 0.3952000141143799]\n\n        # way2 not ok [-0.015382051467895508, 0.5120565295219421, 1.8346221446990967, 0.1779019832611084, -1.937483787536621, -1.5124820470809937, -1.0692400932312012, 0.32760000228881836, -0.11807608604431152, 0.31297802925109863, -0.3911280632019043, 1.4664621353149414, -0.8943638801574707, -0.12114405632019043, -0.11807608604431152, 0.3697359561920166, 0.2530679702758789, -0.09232791513204575, -0.07665801048278809, -0.269942045211792, 1.6951122283935547, -0.4617760181427002, 1.1949440240859985, 0.2025299072265625, 0.3589141368865967, 0.39399999380111694]\n        joints =['HeadYaw', 'HeadPitch', 'LShoulderPitch', 'LShoulderRoll', 'LElbowYaw', 'LElbowRoll', 'LWristYaw',\n                'LHand', 'LHipYawPitch', 'LHipRoll', 'LHipPitch', 'LKneePitch', 'LAnklePitch', 'LAnkleRoll', 'RHipYawPitch',\n                'RHipRoll', 'RHipPitch', 'RKneePitch', 'RAnklePitch', 'RAnkleRoll', 'RShoulderPitch', 'RShoulderRoll',\n                'RElbowYaw', 'RElbowRoll', 'RWristYaw', 'RHand']\n        self.set_joint_angles_list(position, joints)\n\n    def move_in(self):\n        print('move in')\n        LHipRoll_angle = self.joint_angles[self.joint_names.index('LHipRoll')]\n        print(self.joint_names.index('LHipRoll'), LHipRoll_angle)\n        if LHipRoll_angle > 0.45:\n            increment = -0.03\n            self.set_joint_angles(LHipRoll_angle + increment, \"LHipRoll\")\n        else:\n            print('low bound!!!')\n\n    def move_out(self):\n        print('move out')\n        LHipRoll_angle = self.joint_angles[self.joint_names.index('LHipRoll')]\n        #print(self.joint_names.index('LHipRoll'), LHipRoll_angle)\n        if LHipRoll_angle < 0.74:\n            increment = 0.03\n            self.set_joint_angles(LHipRoll_angle + increment, \"LHipRoll\")\n        else:\n            print('upper bound')\n   \n\n    def set_stiffness(self, value):\n        if value == True:\n            service_name = '/body_stiffness/enable'\n        elif value == False:\n            service_name = '/body_stiffness/disable'\n        try:\n            stiffness_service = rospy.ServiceProxy(service_name, Empty)\n            stiffness_service()\n        except rospy.ServiceException, e:\n            rospy.logerr(e)\n\n    def touch_cb(self, data):\n        if data.button == 1 and data.state == 1:  # TB1\n            print(\"move in\")\n            self.move_in()\n        if data.button == 2 and data.state == 1:\n            print(\"move out\")\n            self.move_out()\n        if data.button == 3 and data.state == 1:\n            print(\"kick\")\n            self.kick()\n       \n    # Currently, training is only done in simulation!! -> change position in which it will score a goal in the algorithm class\n    def train(self):\n        for episode in range(100):\n\n            # Get action from optimal policy\n            print(\"Current state: \", self.rldt.state)\n            cur_state = self.rldt.state\n            action = self.rldt.get_action(self.rldt.state)\n\n            # Take action\n            print(\"Next action: \", action)\n           \n            # Determine next state\n            self.rldt.state = self.determine_state(self.rldt.state, action)\n            print(\"New state: \", self.rldt.state)\n            # Determine reward from action taken\n            reward = self.rldt.get_reward(self.rldt.state, action)\n           \n            # Increment visits and update state set\n            self.rldt.visits[int(cur_state), action] += 1         \n            \n            # Update model\n            CH = self.rldt.update_model(cur_state, action, reward, self.rldt.state)\n            exp = self.rldt.check_model(cur_state)\n\n            if CH:\n                self.rldt.compute_values(exp)\n        print(self.rldt.Q)\n\n    def execute(self):\n        #self.rldt.state = 9           # arbitrary test state\n        self.rldt.state = self.leg_state_dis\n        print(\"State before finding kick position: \", self.rldt.state)\n        # get actual state\n        while True:\n            action = self.rldt.get_action(self.rldt.state)\n            print(\"Next action in execution: \", action)\n            self.execute_action(action)\n            if action == 2:\n                print(\"kick state: \", self.rldt.state)\n                break\n            self.rldt.state = self.leg_state_dis          #self.determine_state(self.rldt.state, action)\n            print(\"State: \", self.rldt.state)\n        self.set_initial_stand()\n        \n\n        print(\"final state: \", self.rldt.state)\n\n\n\n    def determine_state(self, state, action):\n        if action == 0:\n            state = state - 1\n        elif action == 1:\n            state = state +1\n        elif action == 2:\n            state = state\n      \n        return state\n\n\n    def execute_action(self, actionIndex):\n        if actionIndex == 0:    #Move In\n            self.move_in()\n        elif actionIndex == 1:\n            self.move_out()\n        elif actionIndex == 2:\n            self.kick()\n        rospy.sleep(1.0)\n\n\n\n\n\n\n\n    def tutorial5_soccer_execute(self):\n        rospy.init_node('tutorial5_soccer_node', anonymous=True)\n        self.set_stiffness(True)\n        self.jointPub = rospy.Publisher(\"joint_angles\", JointAnglesWithSpeed, queue_size=10)\n        self.set_initial_stand()\n        rospy.sleep(2.0)\n        self.one_foot_stand()\n        #rospy.Subscriber(\"tactile_touch\", HeadTouch, self.touch_cb)\n        rospy.Subscriber('joint_states', JointState, self.joints_cb)\n        #rospy.Subscriber(\"/nao_robot/camera/top/camera/image_raw\", Image, self.image_cb)\n        self.train()\n        self.execute()\n        rospy.spin()\n\n\nif __name__=='__main__':\n    node_instance = tutorial5_soccer()\n    node_instance.tutorial5_soccer_execute()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tutorial5_soccer.py b/tutorial5_soccer.py
--- a/tutorial5_soccer.py	
+++ b/tutorial5_soccer.py	
@@ -287,7 +287,7 @@
         rospy.init_node('tutorial5_soccer_node', anonymous=True)
         self.set_stiffness(True)
         self.jointPub = rospy.Publisher("joint_angles", JointAnglesWithSpeed, queue_size=10)
-        self.set_initial_stand()
+        # self.set_initial_stand()
         rospy.sleep(2.0)
         self.one_foot_stand()
         #rospy.Subscriber("tactile_touch", HeadTouch, self.touch_cb)
Index: simulation_gesamt.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from sklearn import tree\nimport numpy as np\n\nSTATES = 10\nEPISODES = 200\n\nclass Central:\n    def __init__(self):\n        # initialize class variables\n        self.joint_names = []\n        self.joint_angles = []\n        self.joint_velocities = []\n        self.jointPub = 0\n        self.stiffness = False  \n        self.key = \"\"\n\n        \n    # read in keyboard \n    def key_cb(self,data):\n        # rospy.loginfo(rospy.get_caller_id() + \"I heard %s\", data.data)\n        self.key = data.data\n\n    def joints_cb(self,data):\n        #rospy.loginfo(\"joint states \"+str(data.name)+str(data.position))\n        # store current joint information in class variables\n        self.joint_names = data.name \n        self.joint_angles = data.position\n        self.joint_velocities = data.velocity\n\n\n    def determine_state(self, state, action):\n        print(\"-----\", state, action)\n        if action == 0:\n            state = state - 1\n        elif action == 1:\n            state = state +1\n        elif action == 2:\n            state = state\n        print(\"-----\", state, action)\n        return state\n\n\n    def central_execute(self, env):\n\n        env.state = 6\n        \n\n           \n        # Main RL-DT loop\n        \n        for episode in range(300):\n\n            # Get action from optimal policy\n            print(\"Current state: \", env.state)\n            cur_state = env.state\n            action = env.get_action(env.state)\n\n            # Take action\n            print(\"Next action: \", action)\n           \n            # Determine next state\n            env.state = self.determine_state(env.state, action)\n            print(\"New state: \", env.state)\n            # Determine reward from action taken\n            reward = env.get_reward(env.state, action)\n           \n            # Increment visits and update state set\n            env.visits[int(cur_state), action] += 1         \n            \n            # Update model\n            CH = env.update_model(cur_state, action, reward, env.state)\n            exp = env.check_model(cur_state)\n\n            if CH:\n                env.compute_values(exp)\n\n        print(env.Q)\n\n    def execute(self, env):\n        env.state = 7           # arbitrary test state\n        for i in range(10):\n            action = env.get_action(env.state)\n            env.state = self.determine_state(env.state, action)\n            \n            \n\nclass algorithm:\n\n    def __init__(self, hip_states):\n        # Total number of states\n        self.Ns_leg = hip_states\n        self.Sm = np.zeros(STATES)\n        self.state = 0\n\n        # Define actions\n       \n        self.possibleActions = [0, 1, 2]\n        \n        # Visit count\n        self.visits = np.zeros((self.Ns_leg, len(self.possibleActions)))\n        \n        # Prob transitions\n        self.Pm = np.zeros((self.Ns_leg, len(self.possibleActions)))\n        self.Rm = np.zeros((self.Ns_leg, len(self.possibleActions)))\n        \n        # Initialize Decision Trees\n        self.transitionTree = tree.DecisionTreeClassifier()\n        self.rewardTree = tree.DecisionTreeClassifier()\n\n        # Initialize input (always same) and output vectors for trees\n        self.x_array = np.zeros(2)\n        self.deltaS = np.array((0))\n        self.deltaR = np.array((0))\n\n        # Define rewards\n        self.goal_reward = 20 # Reward for scoring goal\n        self.miss_penalty = -2 # Miss the goal\n        self.fall_penalty = -20 # Penalty for falling over\n        self.action_penalty = -1 # Penalty for each action execution\n        \n        # Learning parameters\n        self.gamma = 0.001 # Discount factor\n        \n        # Q values for state and action\n        self.Q = np.zeros((self.Ns_leg, len(self.possibleActions)))\n\n        # Rewards for simulation\n        self.stateRewards = [[-20, -1, -20],\n                             [-1, -1, -2],\n                             [-1, -1, 20],\n                             [-1, -1, -2],\n                             [-1, -1, -2],\n                             [-1, -1, -2],\n                             [-1, -1, -2],\n                             [-1, -1, -2],\n                             [-1, -1, -2],\n                             [-1, -20, -20]]\n\n    def allowed_actions(self, s1):\n        # Generate list of actions allowed depending on nao leg state\n        actions_allowed = []\n        if (s1 < self.Ns_leg - 2):  # No passing furthest left kick\n            actions_allowed.append(self.action_dict[\"left\"])\n        if (s1 > 1):  # No passing furthest right kick\n            actions_allowed.append(self.action_dict[\"right\"])\n        actions_allowed.append(self.action_dict[\"kick\"]) # always able to kick\n        actions_allowed = np.array(actions_allowed, dtype=int)\n        return actions_allowed\n    \n    def get_reward(self, state, action):\n        reward = self.stateRewards[state][action]\n        return reward\n                 \n    def get_action(self, state):\n        actionIndex = np.argmax(self.Q[state])\n        # make sure that we do not go out of bounds\n        if state == 0 and actionIndex == 0:\n            if self.Q[state, 1] > self.Q[state, 2]:\n                actionIndex = 1\n            else:\n                actionIndex = 2\n        elif state == 9 and actionIndex == 1:\n            if self.Q[state, 0] > self.Q[state, 2]:\n                actionIndex = 0\n            else:\n                actionIndex = 2\n        return actionIndex\n    \n    def check_model(self, state):\n        exp = np.all(self.Rm[int(state), :] < 0)\n        return exp\n    \n    def add_experience(self, n, state, action, delta):\n        if n == 0:\n            # x (input)\n            x = np.append(np.array(action), state)\n            self.x_array = np.vstack((self.x_array, x))\n\n            # y (output)\n            self.deltaS = np.append(self.deltaS, delta)\n            self.transitionTree = self.transitionTree.fit(self.x_array, self.deltaS)\n       \n        elif n == 2:\n            self.deltaR = np.append(self.deltaR, delta)\n            self.rewardTree = self.rewardTree.fit(self.x_array, self.deltaR)\n        CH = True\n        return CH\n    \n    def combine_results(self, sm1, am):\n        deltaS1_prob = np.max(self.transitionTree.predict_proba([[am, sm1]]))\n        P_deltaS = deltaS1_prob \n\n\n        return P_deltaS\n    \n    def get_predictions(self, sm1, am):\n        deltaR_pred = self.rewardTree.predict([[am, sm1]])\n        return deltaR_pred\n    \n    def update_model(self, state, action, reward, state_):\n        n = 1\n        CH = False\n        xi = np.zeros(n)\n        for i in range(n):\n            xi[i] = state - state_\n            CH = self.add_experience(i, state, action, xi[i])\n\n        CH = self.add_experience(n+1, state, action, reward)\n\n        for sm in range(len(self.Sm)):\n            for am in range(len(self.possibleActions)):\n                self.Pm[sm, am] = self.combine_results(sm, am)\n                self.Rm[sm, am] = self.get_predictions(sm, am)\n        return CH\n    \n    def compute_values(self, exp):\n        minvisits = np.min(self.visits)\n        for sm in range(len(self.Sm)):\n            \n            for am in range(len(self.possibleActions)):\n                if exp and self.visits[sm,am] == minvisits:\n                    self.Q[sm,am] = self.goal_reward\n                else:\n                    self.Q[sm, am] = self.Rm[sm, am]\n                    for sm1_ in range(self.Ns_leg):\n                        \n                        self.Q[sm, am] += self.gamma * self.Pm[sm1_,  am] \\\n                        * np.max(self.Q[sm1_, :])\n                            \n\n                       \n\n\n        \n\nif __name__=='__main__':\n    agent = algorithm(hip_states=STATES)\n    central_instance = Central()\n    central_instance.central_execute(agent)\n    central_instance.execute(agent)\n\n
===================================================================
diff --git a/simulation_gesamt.py b/simulation_gesamt.py
--- a/simulation_gesamt.py	
+++ b/simulation_gesamt.py	
@@ -41,7 +41,8 @@
 
 
     def central_execute(self, env):
-
+        
+        # central start state
         env.state = 6
         
 
@@ -126,14 +127,14 @@
 
         # Rewards for simulation
         self.stateRewards = [[-20, -1, -20],
-                             [-1, -1, -2],
+                             [-1, -1, -20],
+                             [-1, -1, -20],
+                             [-1, -1, -20],
+                             [-1, -1, -20],
+                             [-1, -1, -20],
+                             [-1, -1, -20],
                              [-1, -1, 20],
-                             [-1, -1, -2],
-                             [-1, -1, -2],
-                             [-1, -1, -2],
-                             [-1, -1, -2],
-                             [-1, -1, -2],
-                             [-1, -1, -2],
+                             [-1, -1, -20],
                              [-1, -20, -20]]
 
     def allowed_actions(self, s1):
